# MSIConverter Function Call Flow

## Complete Function-Level Call Tree

This document shows the exact function call sequence when running:
```bash
msiconvert data.imzML output.zarr --pixel-size 25
```

## ğŸš€ Entry Point: CLI Parsing

```
__main__.py:main()
â”œâ”€â”€ argparse.ArgumentParser()
â”œâ”€â”€ parser.parse_args()
â”œâ”€â”€ Path(args.input).resolve()
â”œâ”€â”€ Path(args.output).resolve()
â””â”€â”€ setup_logging(log_level=args.log_level, log_file=args.log_file)
    â””â”€â”€ logging_config.py:setup_logging()
        â”œâ”€â”€ logging.basicConfig()
        â”œâ”€â”€ logging.getLogger().setLevel()
        â””â”€â”€ logging.StreamHandler() / logging.FileHandler()
```

## ğŸ” Format Detection & Reader Setup

```
__main__.py:main()
â””â”€â”€ convert.py:convert_msi()
    â”œâ”€â”€ registry.py:detect_format(input_path)
    â”‚   â”œâ”€â”€ Path(input_path).suffix â†’ ".imzML"
    â”‚   â”œâ”€â”€ for format_name, detector_func in format_detectors.items():
    â”‚   â””â”€â”€ imzml_reader.py:detect_imzml_format()
    â”‚       â”œâ”€â”€ Path(path).suffix.lower() == '.imzml'
    â”‚       â”œâ”€â”€ Path(path).with_suffix('.ibd').exists()
    â”‚       â””â”€â”€ return True
    â”‚
    â”œâ”€â”€ registry.py:get_reader_class("imzml")
    â”‚   â””â”€â”€ return readers["imzml"]  # ImzMLReader class
    â”‚
    â””â”€â”€ ImzMLReader(input_path)
        â”œâ”€â”€ __init__(self, imzml_path)
        â”‚   â”œâ”€â”€ self.imzml_path = Path(imzml_path)
        â”‚   â”œâ”€â”€ self.ibd_path = self.imzml_path.with_suffix('.ibd')
        â”‚   â”œâ”€â”€ pyimzml.ImzMLParser(str(self.imzml_path))
        â”‚   â””â”€â”€ self._create_metadata_extractor()
        â”‚       â””â”€â”€ ImzMLMetadataExtractor(self.parser, self.imzml_path)
        â”‚
        â””â”€â”€ get_essential_metadata()
            â””â”€â”€ metadata_extractor.get_essential()
                â”œâ”€â”€ if self._essential_cache is None:
                â””â”€â”€ _extract_essential_impl()
                    â”œâ”€â”€ np.array(self.parser.coordinates)
                    â”œâ”€â”€ _calculate_dimensions(coords)
                    â”‚   â”œâ”€â”€ coords_0based = coords - 1
                    â”‚   â”œâ”€â”€ max_coords = np.max(coords_0based, axis=0)
                    â”‚   â””â”€â”€ return (max_coords[0]+1, max_coords[1]+1, max_coords[2]+1)
                    â”œâ”€â”€ _calculate_bounds(coords)
                    â”‚   â”œâ”€â”€ x_coords = coords[:, 0].astype(float)
                    â”‚   â”œâ”€â”€ y_coords = coords[:, 1].astype(float)
                    â”‚   â””â”€â”€ return (min(x), max(x), min(y), max(y))
                    â”œâ”€â”€ _get_mass_range_fast()
                    â”‚   â”œâ”€â”€ first_mzs, _ = self.parser.getspectrum(0)
                    â”‚   â”œâ”€â”€ min_mass = float(np.min(first_mzs))
                    â”‚   â”œâ”€â”€ max_mass = float(np.max(first_mzs))
                    â”‚   â”œâ”€â”€ for idx in [n//4, n//2, 3*n//4, n-1]:
                    â”‚   â”‚   â”œâ”€â”€ mzs, _ = self.parser.getspectrum(idx)
                    â”‚   â”‚   â”œâ”€â”€ min_mass = min(min_mass, np.min(mzs))
                    â”‚   â”‚   â””â”€â”€ max_mass = max(max_mass, np.max(mzs))
                    â”‚   â””â”€â”€ return (min_mass, max_mass)
                    â”œâ”€â”€ _extract_pixel_size_fast()
                    â”‚   â”œâ”€â”€ if hasattr(self.parser, 'imzmldict'):
                    â”‚   â”œâ”€â”€ x_size = self.parser.imzmldict.get('pixel size x')
                    â”‚   â”œâ”€â”€ y_size = self.parser.imzmldict.get('pixel size y')
                    â”‚   â””â”€â”€ return (float(x_size), float(y_size)) or None
                    â”œâ”€â”€ _estimate_memory(n_spectra)
                    â”‚   â”œâ”€â”€ avg_peaks_per_spectrum = 1000
                    â”‚   â”œâ”€â”€ bytes_per_value = 8
                    â”‚   â”œâ”€â”€ estimated_bytes = n_spectra * avg_peaks * 2 * bytes_per_value
                    â”‚   â””â”€â”€ return estimated_bytes / (1024**3)
                    â””â”€â”€ return EssentialMetadata(...)
```

## ğŸ“ Pixel Size Detection (if not provided)

```
__main__.py:main()
â””â”€â”€ if args.pixel_size is None:
    â””â”€â”€ detect_pixel_size_interactive(reader, input_format)
        â”œâ”€â”€ reader.get_essential_metadata()  # Unified metadata extraction
        â”œâ”€â”€ essential_metadata.pixel_size  # Check if auto-detected
        â”œâ”€â”€ if pixel_size is None:
        â”‚   â”œâ”€â”€ print("Could not automatically detect pixel size")
        â”‚   â”œâ”€â”€ print("Please enter pixel size manually:")
        â”‚   â”œâ”€â”€ input("Pixel size (micrometers): ")
        â”‚   â”œâ”€â”€ float(user_input)
        â”‚   â””â”€â”€ if pixel_size <= 0: raise ValueError
        â”œâ”€â”€ print(f"Using pixel size: {pixel_size} Î¼m")
        â””â”€â”€ return pixel_size, detection_info, essential_metadata  # Returns metadata for reuse
```

## ğŸ”„ Converter Setup & Initialization

```
convert.py:convert_msi()
â”œâ”€â”€ registry.py:get_converter_class("spatialdata")
â”‚   â””â”€â”€ return converters["spatialdata"]  # SpatialDataConverter class
â”‚
â”œâ”€â”€ # Metadata reuse optimization - skips re-extraction if provided
â”œâ”€â”€ if essential_metadata is None:
â”‚   â””â”€â”€ essential_metadata = reader.get_essential_metadata()
â”‚
â””â”€â”€ SpatialDataConverter(reader, output_path, **kwargs)
    â”œâ”€â”€ __init__(self, reader, output_path, dataset_id, pixel_size_um, handle_3d)
    â”‚   â”œâ”€â”€ self.reader = reader
    â”‚   â”œâ”€â”€ self.output_path = Path(output_path)
    â”‚   â”œâ”€â”€ self.dataset_id = dataset_id
    â”‚   â”œâ”€â”€ self.pixel_size_um = pixel_size_um
    â”‚   â”œâ”€â”€ self.handle_3d = handle_3d
    â”‚   â””â”€â”€ self.batch_size = 1000  # Default batch size
    â”‚
    â””â”€â”€ convert()  # Template method starts here
        â”œâ”€â”€ _initialize_conversion()
        â”œâ”€â”€ _create_data_structures()
        â”œâ”€â”€ _process_spectra(data_structures)
        â”œâ”€â”€ _finalize_data(data_structures)
        â””â”€â”€ _save_output(final_data)
```

## ğŸ—ï¸ Data Structure Creation

```
SpatialDataConverter.convert()
â””â”€â”€ _initialize_conversion()
    â”œâ”€â”€ self.reader.get_essential_metadata()  # Already cached
    â”œâ”€â”€ self.essential_metadata = essential_metadata
    â”œâ”€â”€ self.dimensions = essential_metadata.dimensions
    â”œâ”€â”€ self.pixel_size = essential_metadata.pixel_size or self.pixel_size_um
    â”œâ”€â”€ logger.info(f"Initializing conversion for {self.dimensions} dataset")
    â”‚
    â”œâ”€â”€ self.reader.get_common_mass_axis()
    â”‚   â””â”€â”€ ImzMLReader.get_common_mass_axis()
    â”‚       â”œâ”€â”€ if hasattr(self.parser, 'continuous') and self.parser.continuous:
    â”‚       â”‚   â”œâ”€â”€ mzs, _ = self.parser.getspectrum(0)  # First spectrum
    â”‚       â”‚   â””â”€â”€ return np.array(mzs, dtype=np.float64)
    â”‚       â””â”€â”€ else:  # Processed mode
    â”‚           â”œâ”€â”€ all_mzs = set()
    â”‚           â”œâ”€â”€ for i in range(len(self.parser.coordinates)):
    â”‚           â”‚   â”œâ”€â”€ mzs, _ = self.parser.getspectrum(i)
    â”‚           â”‚   â””â”€â”€ all_mzs.update(mzs)
    â”‚           â””â”€â”€ return np.array(sorted(all_mzs), dtype=np.float64)
    â”‚
    â”œâ”€â”€ self.common_mass_axis = common_mass_axis
    â”œâ”€â”€ logger.info(f"Common mass axis: {len(self.common_mass_axis)} points")
    â”œâ”€â”€ logger.info(f"Mass range: {self.common_mass_axis[0]:.2f} - {self.common_mass_axis[-1]:.2f}")
    â””â”€â”€ logger.info(f"Pixel size: {self.pixel_size} Î¼m")

â””â”€â”€ _create_data_structures()
    â”œâ”€â”€ n_pixels = self.dimensions[0] * self.dimensions[1] * self.dimensions[2]
    â”œâ”€â”€ n_masses = len(self.common_mass_axis)
    â”œâ”€â”€ logger.info(f"Creating sparse matrix: {n_pixels} Ã— {n_masses}")
    â”‚
    â””â”€â”€ return {
        'spectral_matrix': scipy.sparse.lil_matrix((n_pixels, n_masses), dtype=np.float32),
        'coordinates': [],
        'tic_values': np.zeros(n_pixels, dtype=np.float32),
        'valid_pixels': np.zeros(n_pixels, dtype=bool),
        'pixel_count': 0
    }
```

## ğŸ”„ Spectrum Processing Loop

```
SpatialDataConverter.convert()
â””â”€â”€ _process_spectra(data_structures)
    â”œâ”€â”€ batch_size = self.batch_size  # 1000
    â”œâ”€â”€ total_spectra = self.essential_metadata.n_spectra
    â”œâ”€â”€ logger.info(f"Processing {total_spectra} spectra in batches of {batch_size}")
    â”‚
    â””â”€â”€ for batch in self.reader.iter_spectra(batch_size=batch_size):
        â”œâ”€â”€ ImzMLReader.iter_spectra(batch_size)
        â”‚   â”œâ”€â”€ coordinates = self.parser.coordinates
        â”‚   â”œâ”€â”€ batch_count = 0
        â”‚   â”œâ”€â”€ current_batch = []
        â”‚   â”‚
        â”‚   â””â”€â”€ for i, coord in enumerate(coordinates):
        â”‚       â”œâ”€â”€ mzs, intensities = self.parser.getspectrum(i)
        â”‚       â”œâ”€â”€ spectrum_data = {
        â”‚       â”‚   'coordinates': coord,      # (x, y, z)
        â”‚       â”‚   'mzs': np.array(mzs),
        â”‚       â”‚   'intensities': np.array(intensities)
        â”‚       â”‚   }
        â”‚       â”œâ”€â”€ current_batch.append(spectrum_data)
        â”‚       â”œâ”€â”€ if len(current_batch) >= batch_size:
        â”‚       â”‚   â”œâ”€â”€ yield current_batch
        â”‚       â”‚   â””â”€â”€ current_batch = []
        â”‚       â””â”€â”€ if i % 1000 == 0:
        â”‚           â””â”€â”€ logger.debug(f"Loaded {i} spectra")
        â”‚
        â””â”€â”€ for spectrum_data in batch:
            â””â”€â”€ _process_single_spectrum(
                   spectrum_data['coordinates'],
                   spectrum_data['mzs'],
                   spectrum_data['intensities'],
                   data_structures
               )
```

## ğŸ”¬ Single Spectrum Processing

```
_process_single_spectrum(pixel_coords, mzs, intensities, data_structures)
â”œâ”€â”€ x, y, z = pixel_coords  # e.g., (1, 1, 1)
â”œâ”€â”€ pixel_idx = self._coords_to_pixel_index(x, y, z)
â”‚   â”œâ”€â”€ # Convert 3D coordinates to 1D pixel index
â”‚   â”œâ”€â”€ x_idx, y_idx, z_idx = x-1, y-1, z-1  # Convert to 0-based
â”‚   â”œâ”€â”€ dims_x, dims_y, dims_z = self.dimensions
â”‚   â””â”€â”€ return z_idx * (dims_x * dims_y) + y_idx * dims_x + x_idx
â”‚
â”œâ”€â”€ # Map spectrum m/z values to common mass axis indices
â”œâ”€â”€ mass_indices = np.searchsorted(self.common_mass_axis, mzs)
â”‚   # Uses binary search: O(log n) complexity
â”‚
â”œâ”€â”€ # Filter out m/z values outside the common range
â”œâ”€â”€ valid_mask = (mass_indices < len(self.common_mass_axis))
â”œâ”€â”€ mass_indices = mass_indices[valid_mask]
â”œâ”€â”€ intensities = intensities[valid_mask]
â”‚
â”œâ”€â”€ # Store intensities in sparse matrix
â”œâ”€â”€ spectral_matrix = data_structures['spectral_matrix']
â”œâ”€â”€ spectral_matrix[pixel_idx, mass_indices] = intensities
â”‚
â”œâ”€â”€ # Calculate and store Total Ion Current (TIC)
â”œâ”€â”€ tic = np.sum(intensities)
â”œâ”€â”€ data_structures['tic_values'][pixel_idx] = tic
â”‚
â”œâ”€â”€ # Mark pixel as containing data
â”œâ”€â”€ data_structures['valid_pixels'][pixel_idx] = True
â”‚
â”œâ”€â”€ # Store coordinate mapping
â”œâ”€â”€ data_structures['coordinates'].append((x, y, z))
â”œâ”€â”€ data_structures['pixel_count'] += 1
â”‚
â””â”€â”€ # Progress logging
    â””â”€â”€ if data_structures['pixel_count'] % 1000 == 0:
        â””â”€â”€ logger.info(f"Processed {data_structures['pixel_count']} spectra")
```

## ğŸ¯ Data Finalization

```
SpatialDataConverter.convert()
â””â”€â”€ _finalize_data(data_structures)
    â”œâ”€â”€ logger.info("Finalizing data structures...")
    â”‚
    â”œâ”€â”€ # Convert sparse matrix to efficient CSR format
    â”œâ”€â”€ spectral_matrix = data_structures['spectral_matrix'].tocsr()
    â”œâ”€â”€ logger.info(f"Sparse matrix density: {spectral_matrix.nnz / spectral_matrix.size * 100:.2f}%")
    â”‚
    â”œâ”€â”€ # Create observation (pixel) metadata DataFrame
    â”œâ”€â”€ coordinates = data_structures['coordinates']
    â”œâ”€â”€ obs_df = pd.DataFrame({
    â”‚   'x': [c[0] for c in coordinates],
    â”‚   'y': [c[1] for c in coordinates],
    â”‚   'z': [c[2] for c in coordinates],
    â”‚   'tic': data_structures['tic_values'][data_structures['valid_pixels']]
    â”‚   })
    â”‚
    â”œâ”€â”€ # Create variable (mass) metadata DataFrame
    â”œâ”€â”€ var_df = pd.DataFrame({
    â”‚   'mz': self.common_mass_axis,
    â”‚   'mass_index': np.arange(len(self.common_mass_axis))
    â”‚   })
    â”‚
    â”œâ”€â”€ # Create AnnData object (scanpy/single-cell format)
    â”œâ”€â”€ import anndata
    â”œâ”€â”€ adata = anndata.AnnData(
    â”‚   X=spectral_matrix,           # Observations Ã— Variables matrix
    â”‚   obs=obs_df,                  # Pixel metadata
    â”‚   var=var_df,                  # Mass axis metadata
    â”‚   dtype=np.float32
    â”‚   )
    â”‚
    â”œâ”€â”€ # Create pixel boundary shapes
    â”œâ”€â”€ shapes_gdf = self._create_pixel_shapes()
    â”‚   â”œâ”€â”€ import geopandas as gpd
    â”‚   â”œâ”€â”€ from shapely.geometry import Polygon
    â”‚   â”œâ”€â”€ shapes = []
    â”‚   â”œâ”€â”€ for x, y, z in coordinates:
    â”‚   â”‚   â”œâ”€â”€ # Calculate pixel boundaries
    â”‚   â”‚   â”œâ”€â”€ x_min = (x - 1) * self.pixel_size[0]
    â”‚   â”‚   â”œâ”€â”€ x_max = x * self.pixel_size[0]
    â”‚   â”‚   â”œâ”€â”€ y_min = (y - 1) * self.pixel_size[1]
    â”‚   â”‚   â”œâ”€â”€ y_max = y * self.pixel_size[1]
    â”‚   â”‚   â”œâ”€â”€ polygon = Polygon([
    â”‚   â”‚   â”‚   (x_min, y_min), (x_max, y_min),
    â”‚   â”‚   â”‚   (x_max, y_max), (x_min, y_max)
    â”‚   â”‚   â”‚   ])
    â”‚   â”‚   â””â”€â”€ shapes.append(polygon)
    â”‚   â””â”€â”€ return gpd.GeoDataFrame({'geometry': shapes})
    â”‚
    â”œâ”€â”€ # Create TIC image
    â”œâ”€â”€ tic_image = self._create_tic_image(data_structures['tic_values'])
    â”‚   â”œâ”€â”€ import xarray as xr
    â”‚   â”œâ”€â”€ # Reshape TIC values to spatial grid
    â”‚   â”œâ”€â”€ tic_grid = np.zeros(self.dimensions[:2])  # 2D grid
    â”‚   â”œâ”€â”€ for i, (x, y, z) in enumerate(coordinates):
    â”‚   â”‚   â””â”€â”€ tic_grid[y-1, x-1] = data_structures['tic_values'][i]
    â”‚   â”œâ”€â”€ # Create coordinate arrays
    â”‚   â”œâ”€â”€ x_coords = np.arange(self.dimensions[0]) * self.pixel_size[0]
    â”‚   â”œâ”€â”€ y_coords = np.arange(self.dimensions[1]) * self.pixel_size[1]
    â”‚   â””â”€â”€ return xr.DataArray(
    â”‚       tic_grid,
    â”‚       dims=['y', 'x'],
    â”‚       coords={'x': x_coords, 'y': y_coords},
    â”‚       name='total_ion_current'
    â”‚       )
    â”‚
    â””â”€â”€ return {
        'tables': {self.dataset_id: adata},
        'shapes': {f"{self.dataset_id}_shapes": shapes_gdf},
        'images': {f"{self.dataset_id}_tic": tic_image}
    }
```

## ğŸ’¾ Output Generation & Metadata Addition

```
SpatialDataConverter.convert()
â””â”€â”€ _save_output(final_data)
    â”œâ”€â”€ logger.info("Creating SpatialData object...")
    â”œâ”€â”€ import spatialdata as sd
    â”œâ”€â”€ sdata = sd.SpatialData(
    â”‚   tables=final_data['tables'],       # AnnData objects
    â”‚   shapes=final_data['shapes'],       # GeoDataFrames
    â”‚   images=final_data['images']        # xarray DataArrays
    â”‚   )
    â”‚
    â”œâ”€â”€ # Add comprehensive metadata
    â”œâ”€â”€ comprehensive_metadata = self.reader.get_comprehensive_metadata()
    â”‚   â””â”€â”€ ImzMLMetadataExtractor.get_comprehensive()
    â”‚       â”œâ”€â”€ if self._comprehensive_cache is None:
    â”‚       â””â”€â”€ _extract_comprehensive_impl()
    â”‚           â”œâ”€â”€ essential = self.get_essential()  # Already cached
    â”‚           â”œâ”€â”€ format_specific = self._extract_imzml_specific()
    â”‚           â”‚   â”œâ”€â”€ imzml_version = "1.1.0"
    â”‚           â”‚   â”œâ”€â”€ file_mode = "continuous" or "processed"
    â”‚           â”‚   â”œâ”€â”€ ibd_file = str(self.imzml_path.with_suffix('.ibd'))
    â”‚           â”‚   â”œâ”€â”€ spectrum_count = len(self.parser.coordinates)
    â”‚           â”‚   â””â”€â”€ scan_settings = {}
    â”‚           â”œâ”€â”€ acquisition_params = self._extract_acquisition_params()
    â”‚           â”‚   â”œâ”€â”€ if not self.get_essential().has_pixel_size:
    â”‚           â”‚   â”‚   â””â”€â”€ pixel_size = self._extract_pixel_size_from_xml()
    â”‚           â”‚   â””â”€â”€ # Extract scan direction, pattern, etc. from imzmldict
    â”‚           â”œâ”€â”€ instrument_info = self._extract_instrument_info()
    â”‚           â”‚   â””â”€â”€ # Extract instrument model, serial, software, etc.
    â”‚           â”œâ”€â”€ raw_metadata = self._extract_raw_metadata()
    â”‚           â”‚   â””â”€â”€ return dict(self.parser.imzmldict)
    â”‚           â””â”€â”€ return ComprehensiveMetadata(...)
    â”‚
    â”œâ”€â”€ self.add_metadata(sdata, comprehensive_metadata)
    â”‚   â”œâ”€â”€ conversion_info = {
    â”‚   â”‚   'dataset_id': self.dataset_id,
    â”‚   â”‚   'pixel_size_um': self.pixel_size,
    â”‚   â”‚   'conversion_timestamp': datetime.now().isoformat(),
    â”‚   â”‚   'msiconvert_version': msiconvert.__version__,
    â”‚   â”‚   'input_format': 'imzml',
    â”‚   â”‚   'output_format': 'spatialdata'
    â”‚   â”‚   }
    â”‚   â”œâ”€â”€ sdata.metadata['conversion_info'] = conversion_info
    â”‚   â”œâ”€â”€ sdata.metadata['essential_metadata'] = asdict(comprehensive_metadata.essential)
    â”‚   â”œâ”€â”€ sdata.metadata['format_specific'] = comprehensive_metadata.format_specific
    â”‚   â”œâ”€â”€ sdata.metadata['acquisition_params'] = comprehensive_metadata.acquisition_params
    â”‚   â”œâ”€â”€ sdata.metadata['instrument_info'] = comprehensive_metadata.instrument_info
    â”‚   â””â”€â”€ sdata.metadata['raw_metadata'] = comprehensive_metadata.raw_metadata
    â”‚
    â”œâ”€â”€ # Save to Zarr format
    â”œâ”€â”€ logger.info(f"Writing output to {self.output_path}")
    â”œâ”€â”€ sdata.write(str(self.output_path))
    â”‚   â”œâ”€â”€ # SpatialData handles Zarr serialization
    â”‚   â”œâ”€â”€ # Creates .zarr directory with:
    â”‚   â”‚   â”œâ”€â”€ tables/
    â”‚   â”‚   â”‚   â””â”€â”€ {dataset_id}/  # AnnData as Zarr
    â”‚   â”‚   â”œâ”€â”€ shapes/
    â”‚   â”‚   â”‚   â””â”€â”€ {dataset_id}_shapes/  # GeoDataFrame as Parquet
    â”‚   â”‚   â”œâ”€â”€ images/
    â”‚   â”‚   â”‚   â””â”€â”€ {dataset_id}_tic/  # xarray as Zarr
    â”‚   â”‚   â””â”€â”€ .zattrs  # Metadata as JSON
    â”‚   â””â”€â”€ # Zarr uses chunked, compressed storage
    â”‚
    â”œâ”€â”€ logger.info("Conversion completed successfully!")
    â”œâ”€â”€ logger.info(f"Output written to: {self.output_path}")
    â”œâ”€â”€ logger.info(f"Dataset dimensions: {self.dimensions}")
    â”œâ”€â”€ logger.info(f"Number of spectra: {self.essential_metadata.n_spectra}")
    â”œâ”€â”€ logger.info(f"Mass axis points: {len(self.common_mass_axis)}")
    â””â”€â”€ return True
```

## ğŸ“Š Performance Metrics & Cleanup

```
convert.py:convert_msi()
â”œâ”€â”€ conversion_success = converter.convert()
â”œâ”€â”€ reader.close()
â”‚   â””â”€â”€ ImzMLReader.close()
â”‚       â”œâ”€â”€ if hasattr(self.parser, 'close'):
â”‚       â”‚   â””â”€â”€ self.parser.close()
â”‚       â”œâ”€â”€ self.metadata_extractor.clear_cache()
â”‚       â””â”€â”€ logger.debug("ImzML reader closed")
â”‚
â”œâ”€â”€ if conversion_success:
â”‚   â”œâ”€â”€ logger.info("âœ… Conversion completed successfully")
â”‚   â”œâ”€â”€ output_size = sum(f.stat().st_size for f in Path(output_path).rglob('*') if f.is_file())
â”‚   â”œâ”€â”€ logger.info(f"Output size: {output_size / (1024**3):.2f} GB")
â”‚   â””â”€â”€ return True
â”œâ”€â”€ else:
â”‚   â”œâ”€â”€ logger.error("âŒ Conversion failed")
â”‚   â””â”€â”€ return False
```

## Summary: Function Call Statistics

**Total Functions Called**: ~50-100 depending on dataset size and format
**Most Critical Path**:
1. `detect_format()` â†’ `ImzMLReader()` â†’ `get_essential_metadata()`
2. `get_common_mass_axis()` â†’ `iter_spectra()` â†’ `_process_single_spectrum()`
3. `_create_data_structures()` â†’ `_finalize_data()` â†’ `_save_output()`

**Performance Bottlenecks**:
- `iter_spectra()`: I/O bound for large files
- `np.searchsorted()`: CPU bound for mass axis mapping
- `sparse.lil_matrix.tocsr()`: Memory bound for matrix conversion
- `sdata.write()`: I/O bound for Zarr serialization

**Memory Management**:
- Sparse matrices reduce memory by ~95% vs dense arrays
- Batch processing prevents memory overflow on large datasets
- Caching prevents redundant metadata extraction
- Progressive cleanup releases memory after each phase
